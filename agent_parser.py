#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä-–∞–≥–µ–Ω—Ç –¥–ª—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–¥–ª–µ–Ω–∏–∏ –∂–∏–∑–Ω–∏
–†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ PDF —Å—Ç–∞—Ç–µ–π
"""

import requests
import xml.etree.ElementTree as ET
import time
import os
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from urllib.parse import quote, urljoin, urlparse
import re
from datetime import datetime
import hashlib

from pubmed_parser import PubMedParser
from agent_coordinator import AgentCoordinator


class AgentParser(PubMedParser):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä-–∞–≥–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ PDF –∏ —Ä–∞–±–æ—Ç—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–æ–º"""
    
    def __init__(self, coordinator: AgentCoordinator, 
                 email: str = None, api_key: str = None,
                 pdf_dir: str = "longevity_pdfs"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞-–∞–≥–µ–Ω—Ç–∞
        
        Args:
            coordinator: –≠–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
            email: Email –¥–ª—è API PubMed
            api_key: API –∫–ª—é—á PubMed
            pdf_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è PDF —Ñ–∞–π–ª–æ–≤
        """
        super().__init__(email=email, api_key=api_key)
        self.coordinator = coordinator
        self.pdf_dir = Path(pdf_dir)
        self.pdf_dir.mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ PDF –∑–∞–≥—Ä—É–∑–∫–∏
        self.pdf_sources = [
            'pmc',           # PubMed Central
            'doi_redirect',  # –ü–µ—Ä–µ–∞–¥—Ä–µ—Å–∞—Ü–∏—è —á–µ—Ä–µ–∑ DOI
            'unpaywall',     # Unpaywall API
            'arxiv',         # ArXiv –ø—Ä–µ–ø—Ä–∏–Ω—Ç—ã
            'biorxiv'        # BioRxiv –ø—Ä–µ–ø—Ä–∏–Ω—Ç—ã
        ]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞
        self.session_stats = {
            'queries_processed': 0,
            'articles_found': 0,
            'pdfs_attempted': 0,
            'pdfs_downloaded': 0,
            'errors': 0
        }
        
        print(f"ü§ñ –ü–∞—Ä—Å–µ—Ä-–∞–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. PDF –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.pdf_dir}")

    def run_autonomous_session(self, max_queries: int = 20, 
                             download_pdfs: bool = True) -> Dict:
        """
        –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–æ–º
        
        Args:
            max_queries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            download_pdfs: –°–∫–∞—á–∏–≤–∞—Ç—å –ª–∏ PDF —Ñ–∞–π–ª—ã
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        """
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Å–µ—Å—Å–∏–∏: {max_queries} –∑–∞–ø—Ä–æ—Å–æ–≤, PDF={'–í–ö–õ' if download_pdfs else '–í–´–ö–õ'}")
        
        session_start = datetime.now()
        processed_queries = 0
        
        try:
            while processed_queries < max_queries:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –æ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
                queries = self.coordinator.get_next_queries(min(5, max_queries - processed_queries))
                
                if not queries:
                    print("üìã –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                    break
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
                for query_info in queries:
                    try:
                        self._process_single_query(query_info, download_pdfs)
                        processed_queries += 1
                        
                        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        if processed_queries < len(queries):
                            time.sleep(1)
                            
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ {query_info['hash'][:8]}: {e}")
                        self.coordinator.report_query_results(
                            query_info['hash'], [], 0, str(e)
                        )
                        self.session_stats['errors'] += 1
                        continue
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                if processed_queries % 5 == 0:
                    self._print_session_progress()
        
        except KeyboardInterrupt:
            print("\n‚õî –°–µ—Å—Å–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
        except Exception as e:
            print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        session_time = datetime.now() - session_start
        final_stats = self._get_final_stats(session_time, processed_queries)
        
        print(f"\nüèÅ –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {session_time}")
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {processed_queries}")
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {self.session_stats['articles_found']}")
        print(f"üìÅ –°–∫–∞—á–∞–Ω–æ PDF: {self.session_stats['pdfs_downloaded']}")
        
        return final_stats
    
    def _process_single_query(self, query_info: Dict, download_pdfs: bool = True):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        query_hash = query_info['hash']
        query_text = query_info['query']
        theme = query_info['theme']
        
        print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {query_text} (—Ç–µ–º–∞: {theme})")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        articles = self.search_and_fetch(
            query=query_text,
            max_results=query_info['max_results'],
            date_from=query_info.get('date_from'),
            date_to=query_info.get('date_to')
        )
        
        self.session_stats['queries_processed'] += 1
        self.session_stats['articles_found'] += len(articles)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º PDF –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        pdfs_downloaded = 0
        if download_pdfs and articles:
            pdfs_downloaded = self._download_pdfs_for_articles(articles, theme)
        
        # –û—Ç—á–∏—Ç—ã–≤–∞–µ–º—Å—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä—É
        self.coordinator.report_query_results(query_hash, articles, pdfs_downloaded)
        
        print(f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(articles)} —Å—Ç–∞—Ç–µ–π, {pdfs_downloaded} PDF")
    
    def _download_pdfs_for_articles(self, articles: List[Dict], theme: str) -> int:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π
        
        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
            theme: –¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö PDF
        """
        if not articles:
            return 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ç–µ–º—ã
        theme_dir = self.pdf_dir / theme
        theme_dir.mkdir(exist_ok=True)
        
        downloaded_count = 0
        
        print(f"üì• –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É PDF –¥–ª—è {len(articles)} —Å—Ç–∞—Ç–µ–π...")
        
        for i, article in enumerate(articles, 1):
            pmid = article.get('pmid', '')
            doi = article.get('doi', '')
            title = article.get('title', '')[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            if not pmid:
                continue
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∏ —Å–∫–∞—á–∞—Ç—å PDF
                pdf_path = self._download_single_pdf(article, theme_dir)
                
                if pdf_path:
                    downloaded_count += 1
                    print(f"  ‚úÖ {i}/{len(articles)}: PDF —Å–∫–∞—á–∞–Ω - {pdf_path.name}")
                else:
                    print(f"  ‚ùå {i}/{len(articles)}: PDF –Ω–µ –Ω–∞–π–¥–µ–Ω - PMID:{pmid}")
                
                self.session_stats['pdfs_attempted'] += 1
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∑–∫–∞–º–∏
                time.sleep(0.5)
                
            except Exception as e:
                print(f"  ‚ùå {i}/{len(articles)}: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF - {e}")
                continue
        
        self.session_stats['pdfs_downloaded'] += downloaded_count
        return downloaded_count
    
    def _download_single_pdf(self, article: Dict, save_dir: Path) -> Optional[Path]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ PDF –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
        
        Args:
            article: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—å–µ
            save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None
        """
        pmid = article.get('pmid', '')
        doi = article.get('doi', '')
        title = article.get('title', '').replace('/', '_').replace('\\', '_')
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:80]  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        filename = f"PMID_{pmid}_{safe_title}.pdf"
        file_path = save_dir / filename
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω –ª–∏ —É–∂–µ —Ñ–∞–π–ª
        if file_path.exists():
            return file_path
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ PDF
        for source in self.pdf_sources:
            try:
                pdf_url = self._find_pdf_url(article, source)
                if pdf_url:
                    if self._download_pdf_from_url(pdf_url, file_path):
                        return file_path
            except Exception as e:
                continue
        
        return None
    
    def _find_pdf_url(self, article: Dict, source: str) -> Optional[str]:
        """
        –ü–æ–∏—Å–∫ URL PDF —Ñ–∞–π–ª–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            article: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—å–µ
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ –ø–æ–∏—Å–∫–∞ ('pmc', 'doi_redirect', 'unpaywall', etc.)
            
        Returns:
            URL PDF —Ñ–∞–π–ª–∞ –∏–ª–∏ None
        """
        pmid = article.get('pmid', '')
        doi = article.get('doi', '')
        
        if source == 'pmc':
            return self._get_pmc_pdf_url(pmid)
        
        elif source == 'doi_redirect' and doi:
            return self._get_doi_pdf_url(doi)
        
        elif source == 'unpaywall' and doi:
            return self._get_unpaywall_pdf_url(doi)
        
        elif source == 'arxiv':
            return self._get_arxiv_pdf_url(article)
        
        elif source == 'biorxiv':
            return self._get_biorxiv_pdf_url(article)
        
        return None
    
    def _get_pmc_pdf_url(self, pmid: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ PDF URL –∏–∑ PubMed Central"""
        if not pmid:
            return None
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ç–∞—Ç—å—è –≤ PMC
            url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi"
            params = {
                'dbfrom': 'pubmed',
                'db': 'pmc',
                'id': pmid,
                'retmode': 'xml'
            }
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code != 200:
                return None
            
            root = ET.fromstring(response.content)
            pmc_ids = [id_elem.text for id_elem in root.findall('.//Id')]
            
            if len(pmc_ids) > 1:  # –ü–µ—Ä–≤—ã–π ID - —ç—Ç–æ PMID, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - PMC
                pmc_id = pmc_ids[1]
                return f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/pdf/"
        
        except Exception:
            pass
        
        return None
    
    def _get_doi_pdf_url(self, doi: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ PDF URL —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–∞–¥—Ä–µ—Å–∞—Ü–∏—é DOI"""
        if not doi:
            return None
        
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å PDF —á–µ—Ä–µ–∑ sci-hub (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å –ª–µ–≥–∞–ª—å–Ω–æ—Å—Ç—å—é!)
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –ª–µ–≥–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            doi_url = f"https://doi.org/{doi}"
            
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø
            response = requests.head(doi_url, timeout=5, allow_redirects=True)
            if 'pdf' in response.headers.get('content-type', '').lower():
                return doi_url
        
        except Exception:
            pass
        
        return None
    
    def _get_unpaywall_pdf_url(self, doi: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ PDF URL —á–µ—Ä–µ–∑ Unpaywall API"""
        if not doi:
            return None
        
        try:
            url = f"https://api.unpaywall.org/v2/{doi}"
            params = {'email': self.email or 'user@example.com'}
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get('is_oa'):  # –û—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø
                    best_oa = data.get('best_oa_location')
                    if best_oa and best_oa.get('url_for_pdf'):
                        return best_oa['url_for_pdf']
        
        except Exception:
            pass
        
        return None
    
    def _get_arxiv_pdf_url(self, article: Dict) -> Optional[str]:
        """–ü–æ–∏—Å–∫ PDF –Ω–∞ ArXiv"""
        title = article.get('title', '').lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ArXiv —Å—Ç–∞—Ç–µ–π
        if 'arxiv' in title or any(word in title for word in ['preprint', 'submitted']):
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ ArXiv API
            pass
        
        return None
    
    def _get_biorxiv_pdf_url(self, article: Dict) -> Optional[str]:
        """–ü–æ–∏—Å–∫ PDF –Ω–∞ BioRxiv"""
        title = article.get('title', '').lower()
        journal = article.get('journal', '').lower()
        
        if 'biorxiv' in journal or 'biorxiv' in title:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ BioRxiv API
            pass
        
        return None
    
    def _download_pdf_from_url(self, url: str, file_path: Path) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ PDF —Ñ–∞–π–ª–∞ –ø–æ URL
        
        Args:
            url: URL PDF —Ñ–∞–π–ª–∞
            file_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ PDF
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and not url.endswith('.pdf'):
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–Ω–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)
            content_length = response.headers.get('content-length')
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb < 0.1 or size_mb > 50:  # –û—Ç 100KB –¥–æ 50MB
                    return False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
            if file_path.exists() and file_path.stat().st_size > 1000:  # –ú–∏–Ω–∏–º—É–º 1KB
                return True
            else:
                file_path.unlink(missing_ok=True)  # –£–¥–∞–ª—è–µ–º –±–∏—Ç—ã–π —Ñ–∞–π–ª
                return False
        
        except Exception as e:
            # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            file_path.unlink(missing_ok=True)
            return False
    
    def _print_session_progress(self):
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å–µ—Å—Å–∏–∏"""
        stats = self.session_stats
        success_rate = (stats['pdfs_downloaded'] / max(stats['pdfs_attempted'], 1)) * 100
        
        print(f"\nüìä –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['queries_processed']}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {stats['articles_found']}")
        print(f"   PDF –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {stats['pdfs_downloaded']}/{stats['pdfs_attempted']} ({success_rate:.1f}%)")
        print(f"   –û—à–∏–±–æ–∫: {stats['errors']}")
    
    def _get_final_stats(self, session_time, processed_queries: int) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Å—Å–∏–∏"""
        coordinator_stats = self.coordinator.get_session_stats()
        
        return {
            'session_time': str(session_time),
            'queries_processed': processed_queries,
            'articles_found': self.session_stats['articles_found'],
            'pdfs_downloaded': self.session_stats['pdfs_downloaded'],
            'pdf_success_rate': (self.session_stats['pdfs_downloaded'] / 
                               max(self.session_stats['pdfs_attempted'], 1)) * 100,
            'errors': self.session_stats['errors'],
            'coordinator_stats': coordinator_stats,
            'avg_articles_per_query': self.session_stats['articles_found'] / max(processed_queries, 1)
        }
    
    def get_downloaded_pdfs_info(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö PDF —Ñ–∞–π–ª–∞—Ö"""
        pdf_info = []
        
        for theme_dir in self.pdf_dir.iterdir():
            if theme_dir.is_dir():
                for pdf_file in theme_dir.glob("*.pdf"):
                    try:
                        stat = pdf_file.stat()
                        pdf_info.append({
                            'theme': theme_dir.name,
                            'filename': pdf_file.name,
                            'size_mb': round(stat.st_size / (1024 * 1024), 2),
                            'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                            'path': str(pdf_file)
                        })
                    except Exception:
                        continue
        
        return sorted(pdf_info, key=lambda x: x['created'], reverse=True)
    
    def cleanup_failed_downloads(self):
        """–û—á–∏—Å—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö/–±–∏—Ç—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫"""
        cleaned = 0
        
        for theme_dir in self.pdf_dir.iterdir():
            if theme_dir.is_dir():
                for pdf_file in theme_dir.glob("*.pdf"):
                    try:
                        # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã (–≤–µ—Ä–æ—è—Ç–Ω–æ –±–∏—Ç—ã–µ)
                        if pdf_file.stat().st_size < 1000:  # –ú–µ–Ω—å—à–µ 1KB
                            pdf_file.unlink()
                            cleaned += 1
                    except Exception:
                        continue
        
        if cleaned > 0:
            print(f"üßπ –û—á–∏—â–µ–Ω–æ {cleaned} –±–∏—Ç—ã—Ö PDF —Ñ–∞–π–ª–æ–≤")
        
        return cleaned


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞-–∞–≥–µ–Ω—Ç–∞
    from agent_coordinator import AgentCoordinator
    
    print("ü§ñ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–ê–†–°–ï–†–ê-–ê–ì–ï–ù–¢–ê")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ –∏ –ø–∞—Ä—Å–µ—Ä–∞
    coordinator = AgentCoordinator()
    parser = AgentParser(coordinator, email="your_email@example.com")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –¥–µ–º–æ-—Å–µ—Å—Å–∏—é
    stats = parser.run_autonomous_session(max_queries=3, download_pdfs=True)
    
    print("\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö PDF:")
    pdf_info = parser.get_downloaded_pdfs_info()
    for info in pdf_info[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
        print(f"  - {info['filename']} ({info['size_mb']} MB) - {info['theme']}")
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
    coordinator.close_session() 